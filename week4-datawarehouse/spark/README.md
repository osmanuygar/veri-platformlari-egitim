# Apache Spark - Big Data Processing

Apache Spark ile distributed data processing.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Spark Cluster'Ä± BaÅŸlat

```bash
# Spark klasÃ¶rÃ¼ne git
cd spark

# Cluster'Ä± baÅŸlat (Master + Worker)
docker-compose up -d

# LoglarÄ± izle
docker-compose logs -f

# Durumu kontrol et
docker-compose ps
```

### 2. Web UI'lara EriÅŸ

```
Spark Master UI: http://localhost:8080
Spark Worker UI: http://localhost:8081
```

### 3. Job Ã‡alÄ±ÅŸtÄ±r

```bash
# PostgreSQL â†’ Parquet ETL
docker exec -it spark_master python /opt/spark-jobs/etl_postgres_to_parquet.py

# MinIO'dan okuma ve analiz
docker exec -it spark_master python /opt/spark-jobs/read_from_minio.py

# SatÄ±ÅŸ aggregation
docker exec -it spark_master python /opt/spark-jobs/sales_aggregation.py
```

## ğŸ› ï¸ KullanÄ±m SeÃ§enekleri

### SeÃ§enek 1: DoÄŸrudan Python (Kolay)
```bash
docker exec -it spark_master python /opt/spark-jobs/etl_postgres_to_parquet.py
```

### SeÃ§enek 2: Spark Submit (Distributed)
```bash
docker exec -it spark_master spark-submit \
  --master spark://spark-master:7077 \
  --executor-memory 1G \
  --total-executor-cores 2 \
  /opt/spark-jobs/etl_postgres_to_parquet.py
```

### SeÃ§enek 3: PySpark Shell (Ä°nteraktif)
```bash
docker exec -it spark_master pyspark \
  --master spark://spark-master:7077
```

## ğŸ“¦ Ã–rnek Job'lar

### 1. PostgreSQL â†’ Parquet ETL
PostgreSQL'den veri Ã§eker, Parquet'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve MinIO'ya yazar:
```bash
docker exec -it spark_master \
  python /opt/spark-jobs/etl_postgres_to_parquet.py
```

**Ne yapar:**
- customers, products, orders, order_items tablolarÄ±nÄ± okur
- Parquet formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
- MinIO'nun `processed-data` bucket'Ä±na yazar

### 2. MinIO'dan Okuma ve Analiz
MinIO'daki Parquet dosyalarÄ±nÄ± okur ve analiz eder:
```bash
docker exec -it spark_master \
  python /opt/spark-jobs/read_from_minio.py
```

**Ne yapar:**
- MÃ¼ÅŸteri analizi (ÅŸehir/Ã¼lke bazÄ±nda)
- ÃœrÃ¼n analizi (kategori, fiyat)
- SipariÅŸ analizi (durum, Ã¶deme)
- SipariÅŸ detay analizi

### 3. SatÄ±ÅŸ Aggregation
SatÄ±ÅŸ verilerini aggregate eder (gÃ¼nlÃ¼k, aylÄ±k):
```bash
docker exec -it spark_master \
  python /opt/spark-jobs/sales_aggregation.py
```

**Ne yapar:**
- GÃ¼nlÃ¼k satÄ±ÅŸ toplamlarÄ±
- ÃœrÃ¼n bazÄ±nda satÄ±ÅŸlar
- Kategori bazÄ±nda satÄ±ÅŸlar
- MÃ¼ÅŸteri bazÄ±nda satÄ±ÅŸlar
- Running totals (Window function)
- SonuÃ§larÄ± MinIO'ya kaydeder

## ğŸ’» PySpark ile KullanÄ±m

```python
from pyspark.sql import SparkSession

# Spark session oluÅŸtur
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# PostgreSQL'den oku
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://postgres-oltp:5432/ecommerce_oltp") \
    .option("dbtable", "customers") \
    .option("user", "oltp_user") \
    .option("password", "oltp_pass123") \
    .load()

# Parquet olarak kaydet
df.write.mode("overwrite").parquet("s3a://datawarehouse/customers/")

spark.stop()
```

## ğŸ”§ S3/MinIO KonfigÃ¼rasyonu

Spark'Ä±n MinIO ile Ã§alÄ±ÅŸmasÄ± iÃ§in gerekli ayarlar:

```python
spark = SparkSession.builder \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minio_admin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minio_password123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()
```

## ğŸ“Š Spark DataFrame Operations

```python
# Okuma
df = spark.read.parquet("s3a://datawarehouse/sales/")

# Transformations
result = df.groupBy("product_id") \
    .agg({"quantity": "sum", "total_amount": "sum"}) \
    .orderBy("sum(total_amount)", ascending=False)

# Yazma
result.write.mode("overwrite").parquet("s3a://processed-data/top_products/")
```

## ğŸ³ Docker Compose DetaylarÄ±

### Servisler
- **spark-master**: Spark master node (port 8080, 7077)
- **spark-worker**: Spark worker node (port 8081)
- **spark-jobs**: (Opsiyonel) Custom dependencies ile job container

### Kaynaklar
- Worker Memory: 2GB
- Worker Cores: 2
- Master UI: 8080
- Worker UI: 8081

### Profiller
```bash
# Sadece Master ve Worker
docker-compose up -d

# Custom jobs container ile
docker-compose --profile jobs up -d
```

## ğŸ” Monitoring ve Debug

### Cluster Durumu
```bash
# Container durumu
docker-compose ps

# Master loglarÄ±
docker-compose logs -f spark-master

# Worker loglarÄ±
docker-compose logs -f spark-worker

# Kaynak kullanÄ±mÄ±
docker stats spark_master spark_worker
```

### Web UI'dan Ä°zleme
- **Master UI (8080)**: Aktif worker'lar, running applications
- **Worker UI (8081)**: Executor'lar, resource kullanÄ±mÄ±
- **Application UI (4040)**: Job detaylarÄ± (job Ã§alÄ±ÅŸÄ±rken aktif)

## ğŸ›‘ Durdurma ve Temizlik

```bash
# Durdur
docker-compose down

# Volume'ler ile birlikte sil
docker-compose down -v

# Yeniden baÅŸlat
docker-compose restart
```

## ğŸ¯ Ã–zellikler

- âœ… Spark Standalone Cluster (Master + Worker)
- âœ… PostgreSQL JDBC connector
- âœ… MinIO/S3 entegrasyonu (S3A)
- âœ… Parquet okuma/yazma
- âœ… DataFrame API
- âœ… SQL sorgularÄ±
- âœ… Window functions
- âœ… Web UI monitoring

## ğŸ“š Ã–ÄŸrenme KaynaklarÄ±

### Temel
- [Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API](https://spark.apache.org/docs/latest/api/python/)

### Ä°leri Seviye
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)

## ğŸ’¡ Ä°puÃ§larÄ±

1. **KÃ¼Ã§Ã¼k veri iÃ§in**: DoÄŸrudan Python ile Ã§alÄ±ÅŸtÄ±rÄ±n
2. **BÃ¼yÃ¼k veri iÃ§in**: Spark submit kullanÄ±n
3. **GeliÅŸtirme**: PySpark shell ile interaktif test edin
4. **Production**: Cluster mode ve resource ayarlarÄ±nÄ± optimize edin

## ğŸ”— BaÄŸÄ±mlÄ±lÄ±klar

Bu Spark cluster'Ä± Week 4 eÄŸitiminin parÃ§asÄ±dÄ±r ve ÅŸunlarÄ± gerektirir:
- âœ… PostgreSQL (OLTP) - Kaynak veri
- âœ… MinIO - Data Lake storage
- âœ… `datawarehouse_network` - Docker network

Ana docker-compose.yml ile birlikte Ã§alÄ±ÅŸacak ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r.