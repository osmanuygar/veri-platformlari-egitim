# Hafta 4: Veri AmbarlarÄ±, Veri GÃ¶lleri ve Mimariler

## ğŸ“š Ä°Ã§indekiler

1. [OLTP vs OLAP](#1-oltp-vs-olap)
2. [Veri AmbarÄ± (Data Warehouse)](#2-veri-ambarÄ±-data-warehouse)
3. [Boyutsal Modelleme: Star ve Snowflake ÅemalarÄ±](#3-boyutsal-modelleme-star-ve-snowflake-ÅŸemalarÄ±)
4. [ETL vs ELT](#4-etl-vs-elt)
5. [Veri GÃ¶lÃ¼ (Data Lake)](#5-veri-gÃ¶lÃ¼-data-lake)
6. [Modern Veri Mimarileri](#6-modern-veri-mimarileri)
7. [Veri Kalitesi ve YÃ¶netiÅŸimi](#7-veri-kalitesi-ve-yÃ¶netiÅŸimi)
8. [Veri GÃ¶rselleÅŸtirme](#8-veri-gÃ¶rselleÅŸtirme)
9. [Pratik Uygulamalar](#9-pratik-uygulamalar)
10. [AlÄ±ÅŸtÄ±rmalar](#10-alÄ±ÅŸtÄ±rmalar)

---

## 1. OLTP vs OLAP

### 1.1 OLTP (Online Transaction Processing)

**TanÄ±m:** GÃ¼nlÃ¼k operasyonel iÅŸlemlerin yÃ¼rÃ¼tÃ¼ldÃ¼ÄŸÃ¼ sistem

#### Ã–zellikler
```
âœ… KÄ±sa, hÄ±zlÄ± transaction'lar
âœ… INSERT, UPDATE, DELETE aÄŸÄ±rlÄ±klÄ±
âœ… Ã‡ok sayÄ±da kullanÄ±cÄ±
âœ… NormalizeÅŸmiÅŸ veri
âœ… AnlÄ±k veri tutarlÄ±lÄ±ÄŸÄ±
âœ… DÃ¼ÅŸÃ¼k latency (< 100ms)
```

#### Ã–rnek Sorgular
```sql
-- SipariÅŸ oluÅŸturma
INSERT INTO orders (customer_id, order_date, total)
VALUES (12345, CURRENT_DATE, 450.00);

-- Stok gÃ¼ncelleme
UPDATE products 
SET stock_quantity = stock_quantity - 5
WHERE product_id = 789;

-- MÃ¼ÅŸteri bilgisi gÃ¶rÃ¼ntÃ¼leme
SELECT * FROM customers WHERE customer_id = 12345;
```

#### KullanÄ±m AlanlarÄ±
- E-ticaret siteleri
- Banka iÅŸlemleri
- CRM sistemleri
- Rezervasyon sistemleri
- ERP uygulamalarÄ±

### 1.2 OLAP (Online Analytical Processing)

**TanÄ±m:** Analiz ve raporlama iÃ§in optimize edilmiÅŸ sistem

#### Ã–zellikler
```
âœ… Kompleks, uzun sorgular
âœ… SELECT (okuma) aÄŸÄ±rlÄ±klÄ±
âœ… Az sayÄ±da kullanÄ±cÄ± (analistler)
âœ… Denormalize veri
âœ… Tarihsel veri
âœ… YÃ¼ksek throughput
```

#### Ã–rnek Sorgular
```sql
-- YÄ±llÄ±k satÄ±ÅŸ trendi
SELECT 
    EXTRACT(YEAR FROM order_date) as year,
    EXTRACT(MONTH FROM order_date) as month,
    SUM(total_amount) as monthly_sales,
    COUNT(DISTINCT customer_id) as unique_customers
FROM orders
WHERE order_date >= '2020-01-01'
GROUP BY year, month
ORDER BY year, month;

-- ÃœrÃ¼n kategorisi analizi
SELECT 
    c.category_name,
    COUNT(DISTINCT o.order_id) as order_count,
    SUM(oi.quantity * oi.unit_price) as revenue,
    AVG(oi.quantity * oi.unit_price) as avg_order_value
FROM categories c
JOIN products p ON c.category_id = p.category_id
JOIN order_items oi ON p.product_id = oi.product_id
JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_date >= CURRENT_DATE - INTERVAL '1 year'
GROUP BY c.category_name
ORDER BY revenue DESC;
```

#### KullanÄ±m AlanlarÄ±
- Ä°ÅŸ zekasÄ± (BI) dashboards
- SatÄ±ÅŸ analizleri
- Trend raporlarÄ±
- Veri madenciliÄŸi
- Forecasting

### 1.3 DetaylÄ± KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | OLTP | OLAP                           |
|---------|------|--------------------------------|
| **AmaÃ§** | Operasyonel | Analitik                       |
| **Ä°ÅŸlem TÃ¼rÃ¼** | INSERT, UPDATE, DELETE | SELECT (kompleks)              |
| **Veri Hacmi** | GB - TB | TB - PB                        |
| **Sorgu KarmaÅŸÄ±klÄ±ÄŸÄ±** | Basit | Kompleks                       |
| **Response Time** | Milisaniyeler | Saniyeler-Dakikalar            |
| **KullanÄ±cÄ± SayÄ±sÄ±** | Binlerce | Onlarca                        |
| **Veri GÃ¼ncelliÄŸi** | GerÃ§ek zamanlÄ± | Periyodik (gÃ¼nlÃ¼k, saatlik)    |
| **Normalizasyon** | YÃ¼ksek (3NF) | DÃ¼ÅŸÃ¼k (Denormalize)            |
| **Yedekleme** | SÄ±k (her gÃ¼n) | Nadiren                        |
| **Veri YaÅŸÄ±** | GÃ¼ncel (son 3-12 ay) | Tarihsel (yÄ±llar)              |
| **Ã–rnek** | MySQL, PostgreSQL | Clcikhouse, Redshift, BigQuery |

### 1.4 OLTP'den OLAP'a Veri AkÄ±ÅŸÄ±

```
OLTP (Operasyonel DB)
    â†“ ETL/ELT
Data Warehouse (OLAP)
    â†“ Transformation
Data Marts (Departman bazlÄ±)
    â†“ Visualization
BI Dashboards
```

---

## 2. Veri AmbarÄ± (Data Warehouse)

### 2.1 TanÄ±m

**Data Warehouse:** FarklÄ± kaynaklardan gelen verilerin birleÅŸtirildiÄŸi, analiz ve raporlama iÃ§in optimize edilmiÅŸ merkezi veri deposu.

#### Bill Inmon'un TanÄ±mÄ± (1990)
```
Veri AmbarÄ±:
1. Subject-Oriented (Konu odaklÄ±)
2. Integrated (Entegre)
3. Time-Variant (Zaman bazlÄ±)
4. Non-Volatile (DeÄŸiÅŸmez)
```

### 2.2 Ã–zellikler

#### 1. Subject-Oriented (Konu OdaklÄ±)
```
âŒ OLTP: Uygulama odaklÄ± (SipariÅŸ sistemi, Stok sistemi)
âœ… OLAP: Ä°ÅŸ konusu odaklÄ± (SatÄ±ÅŸlar, MÃ¼ÅŸteriler, ÃœrÃ¼nler)
```

#### 2. Integrated (Entegre)
```
FarklÄ± Kaynaklar:
- CRM (Salesforce)
- ERP (SAP)
- E-ticaret platformu
- Excel dosyalarÄ±
â†“ ETL
Tek, tutarlÄ± format
```

#### 3. Time-Variant (Zaman BazlÄ±)
```sql
-- Her kayÄ±t zaman damgasÄ± iÃ§erir
CREATE TABLE dim_customer (
    customer_key INT PRIMARY KEY,
    customer_id INT,
    name VARCHAR(100),
    valid_from DATE,
    valid_to DATE,
    is_current BOOLEAN
);
```

#### 4. Non-Volatile (DeÄŸiÅŸmez)
```
âŒ OLTP: UPDATE/DELETE sÄ±k
âœ… OLAP: Sadece INSERT (geÃ§miÅŸ korunur)
```

### 2.3 Veri AmbarÄ± Mimarisi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Data Sources                 â”‚
â”‚  (OLTP, Files, APIs, Streams)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ETL/ELT Layer                â”‚
â”‚  (Extract, Transform, Load)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Staging Area                   â”‚
â”‚  (Temporary storage)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Data Warehouse (Core)            â”‚
â”‚  - Fact Tables                      â”‚
â”‚  - Dimension Tables                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Data Marts                    â”‚
â”‚  (Department-specific)              â”‚
â”‚  Sales Mart | Marketing Mart        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BI Tools & Reports               â”‚
â”‚  (Tableau, Power BI, Looker)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.4 PopÃ¼ler Data Warehouse Ã‡Ã¶zÃ¼mleri

#### Snowflake
```
âœ… Cloud-native
âœ… AyrÄ±k storage ve compute
âœ… Otomatik scaling
âœ… Zero-copy cloning
âœ… Time travel
âŒ PahalÄ± olabilir
```

#### Amazon Redshift
```
âœ… AWS entegrasyonu
âœ… Columnar storage
âœ… Massively parallel
âœ… Spectrum (S3 query)
âŒ Manual scaling
```

#### Google BigQuery
```
âœ… Serverless
âœ… Petabyte Ã¶lÃ§eÄŸi
âœ… ML entegrasyonu
âœ… Streaming insert
âŒ SQL dialect farklÄ±
```

#### Azure Synapse Analytics
```
âœ… OLAP + OLTP (Hybrid)
âœ… Azure entegrasyonu
âœ… Spark integration
âœ… Serverless veya provisioned
```

---

## 3. Boyutsal Modelleme: Star ve Snowflake ÅemalarÄ±

### 3.1 Boyutsal Modelleme Nedir?

**TanÄ±m:** OLAP iÃ§in optimize edilmiÅŸ veri modelleme tekniÄŸi. Ralph Kimball tarafÄ±ndan geliÅŸtirildi.

#### Temel Kavramlar

**Fact Table (Olgu Tablosu):**
- Ä°ÅŸlem/olay verileri
- Ã–lÃ§Ã¼lebilir metrikler (sales, quantity, amount)
- Foreign key'ler (boyutlara referans)
- Ã‡ok sayÄ±da satÄ±r (milyonlar, milyarlar)

**Dimension Table (Boyut Tablosu):**
- TanÄ±mlayÄ±cÄ± bilgiler
- Filtreleme ve gruplama iÃ§in
- Az sayÄ±da satÄ±r (binler)
- Zengin metin alanlarÄ±

### 3.2 Star Schema (YÄ±ldÄ±z ÅemasÄ±)

#### YapÄ±
```
      Dim_Date
          â†“
Dim_Customer â†’ FACT_Sales â† Dim_Product
          â†‘
      Dim_Store
```

#### Ã–rnek: E-Ticaret SatÄ±ÅŸ Veri AmbarÄ±

**Fact Tablosu:**
```sql
CREATE TABLE fact_sales (
    sale_id BIGINT PRIMARY KEY,
    
    -- Foreign Keys (Boyutlara referanslar)
    date_key INT REFERENCES dim_date(date_key),
    customer_key INT REFERENCES dim_customer(customer_key),
    product_key INT REFERENCES dim_product(product_key),
    store_key INT REFERENCES dim_store(store_key),
    
    -- Metrikler (Measures)
    quantity INT NOT NULL,
    unit_price DECIMAL(10,2) NOT NULL,
    discount_amount DECIMAL(10,2) DEFAULT 0,
    tax_amount DECIMAL(10,2) NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    
    -- Degenerate Dimension
    order_number VARCHAR(50),
    
    -- Audit columns
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Columnstore index (OLAP iÃ§in)
CREATE INDEX idx_fact_sales_columnstore 
ON fact_sales (date_key, customer_key, product_key, store_key);
```

**Boyut TablolarÄ±:**

```sql
-- Zaman Boyutu
CREATE TABLE dim_date (
    date_key INT PRIMARY KEY,
    full_date DATE NOT NULL,
    day_of_week VARCHAR(10),
    day_of_month INT,
    day_of_year INT,
    week_of_year INT,
    month_name VARCHAR(10),
    month_number INT,
    quarter INT,
    year INT,
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name VARCHAR(50)
);

-- MÃ¼ÅŸteri Boyutu
CREATE TABLE dim_customer (
    customer_key INT PRIMARY KEY,
    customer_id INT, -- Business key
    full_name VARCHAR(100),
    email VARCHAR(100),
    phone VARCHAR(20),
    birth_date DATE,
    age_group VARCHAR(20), -- '18-25', '26-35', etc.
    gender VARCHAR(10),
    
    -- Address
    address_line1 VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    postal_code VARCHAR(20),
    country VARCHAR(50),
    
    -- Segmentation
    customer_segment VARCHAR(30), -- 'VIP', 'Regular', 'New'
    lifetime_value DECIMAL(12,2),
    
    -- SCD Type 2 (Slowly Changing Dimension)
    valid_from DATE,
    valid_to DATE,
    is_current BOOLEAN DEFAULT TRUE,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ÃœrÃ¼n Boyutu
CREATE TABLE dim_product (
    product_key INT PRIMARY KEY,
    product_id INT, -- Business key
    product_name VARCHAR(200),
    product_description TEXT,
    sku VARCHAR(50),
    
    -- Hierarchy
    category_name VARCHAR(50),
    subcategory_name VARCHAR(50),
    brand_name VARCHAR(50),
    
    -- Attributes
    color VARCHAR(30),
    size VARCHAR(20),
    weight DECIMAL(10,2),
    
    -- Pricing
    list_price DECIMAL(10,2),
    cost_price DECIMAL(10,2),
    margin_percent DECIMAL(5,2),
    
    -- Status
    is_active BOOLEAN DEFAULT TRUE,
    
    valid_from DATE,
    valid_to DATE,
    is_current BOOLEAN DEFAULT TRUE
);

-- MaÄŸaza Boyutu
CREATE TABLE dim_store (
    store_key INT PRIMARY KEY,
    store_id INT,
    store_name VARCHAR(100),
    store_type VARCHAR(30), -- 'Physical', 'Online'
    
    -- Location
    address VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    postal_code VARCHAR(20),
    country VARCHAR(50),
    region VARCHAR(50),
    
    -- Details
    manager_name VARCHAR(100),
    opening_date DATE,
    square_meters INT,
    
    is_active BOOLEAN DEFAULT TRUE
);
```

#### Ã–rnek Analitik Sorgular

```sql
-- AylÄ±k satÄ±ÅŸ trendi
SELECT 
    d.year,
    d.month_name,
    COUNT(DISTINCT f.sale_id) as total_orders,
    SUM(f.quantity) as total_quantity,
    SUM(f.total_amount) as total_revenue,
    AVG(f.total_amount) as avg_order_value
FROM fact_sales f
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024
GROUP BY d.year, d.month_number, d.month_name
ORDER BY d.month_number;

-- En Ã§ok satan Ã¼rÃ¼nler (kategori bazÄ±nda)
SELECT 
    p.category_name,
    p.product_name,
    SUM(f.quantity) as units_sold,
    SUM(f.total_amount) as revenue,
    AVG(f.unit_price) as avg_price
FROM fact_sales f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024
GROUP BY p.category_name, p.product_name
ORDER BY revenue DESC
LIMIT 20;

-- MÃ¼ÅŸteri segmenti analizi
SELECT 
    c.customer_segment,
    c.age_group,
    COUNT(DISTINCT c.customer_key) as customer_count,
    COUNT(f.sale_id) as order_count,
    SUM(f.total_amount) as total_revenue,
    AVG(f.total_amount) as avg_order_value,
    SUM(f.total_amount) / COUNT(DISTINCT c.customer_key) as revenue_per_customer
FROM fact_sales f
JOIN dim_customer c ON f.customer_key = c.customer_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024 AND c.is_current = TRUE
GROUP BY c.customer_segment, c.age_group
ORDER BY total_revenue DESC;

-- BÃ¶lgesel performans
SELECT 
    s.region,
    s.city,
    d.quarter,
    COUNT(DISTINCT f.sale_id) as orders,
    SUM(f.total_amount) as revenue,
    SUM(f.quantity) as units_sold
FROM fact_sales f
JOIN dim_store s ON f.store_key = s.store_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE d.year = 2024
GROUP BY s.region, s.city, d.quarter
ORDER BY revenue DESC;
```

#### Star Schema AvantajlarÄ±
```
âœ… Basit ve anlaÅŸÄ±lÄ±r
âœ… HÄ±zlÄ± sorgular
âœ… Ä°yi BI tool desteÄŸi
âœ… Az JOIN
âŒ Veri tekrarÄ± (denormalize)
âŒ Daha fazla storage
```

### 3.3 Snowflake Schema (Kar Tanesi ÅemasÄ±)

#### YapÄ±
```
                    Dim_Date
                        â†“
Dim_Customer â†’ FACT_Sales â† Dim_Product
     â†“                            â†“
Dim_City                    Dim_Category
     â†“                            â†“
Dim_State                   Dim_Subcategory
     â†“
Dim_Country
```

#### Ã–rnek: Normalize EdilmiÅŸ Boyutlar

```sql
-- ÃœrÃ¼n boyutu (normalize)
CREATE TABLE dim_product (
    product_key INT PRIMARY KEY,
    product_id INT,
    product_name VARCHAR(200),
    sku VARCHAR(50),
    subcategory_key INT REFERENCES dim_subcategory(subcategory_key),
    brand_key INT REFERENCES dim_brand(brand_key),
    list_price DECIMAL(10,2)
);

-- Alt kategori tablosu
CREATE TABLE dim_subcategory (
    subcategory_key INT PRIMARY KEY,
    subcategory_name VARCHAR(50),
    category_key INT REFERENCES dim_category(category_key)
);

-- Kategori tablosu
CREATE TABLE dim_category (
    category_key INT PRIMARY KEY,
    category_name VARCHAR(50),
    department_key INT REFERENCES dim_department(department_key)
);

-- Departman tablosu
CREATE TABLE dim_department (
    department_key INT PRIMARY KEY,
    department_name VARCHAR(50)
);

-- Marka tablosu
CREATE TABLE dim_brand (
    brand_key INT PRIMARY KEY,
    brand_name VARCHAR(50),
    brand_country VARCHAR(50)
);
```

#### Snowflake Schema ile Sorgu

```sql
-- Departman bazÄ±nda satÄ±ÅŸlar (Ã§oklu JOIN)
SELECT 
    dept.department_name,
    cat.category_name,
    subcat.subcategory_name,
    SUM(f.total_amount) as revenue
FROM fact_sales f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_subcategory subcat ON p.subcategory_key = subcat.subcategory_key
JOIN dim_category cat ON subcat.category_key = cat.category_key
JOIN dim_department dept ON cat.department_key = dept.department_key
GROUP BY dept.department_name, cat.category_name, subcat.subcategory_name
ORDER BY revenue DESC;
```

#### Snowflake Schema AvantajlarÄ±
```
âœ… Az veri tekrarÄ±
âœ… Kolay gÃ¼ncelleme
âœ… Storage tasarrufu
âŒ Kompleks sorgular
âŒ Daha fazla JOIN
âŒ Daha yavaÅŸ
```

### 3.4 Star vs Snowflake KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | Star Schema | Snowflake Schema |
|---------|-------------|------------------|
| **Normalizasyon** | DÃ¼ÅŸÃ¼k (denormalize) | YÃ¼ksek (normalize) |
| **JOIN SayÄ±sÄ±** | Az | Ã‡ok |
| **Sorgu HÄ±zÄ±** | HÄ±zlÄ± | YavaÅŸ |
| **Storage** | Fazla | Az |
| **KarmaÅŸÄ±klÄ±k** | Basit | KarmaÅŸÄ±k |
| **BakÄ±m** | Kolay | Orta |
| **Tercih** | Ã‡oÄŸu durumda | Nadiren |

### 3.5 Slowly Changing Dimensions (SCD)

#### Type 0: DeÄŸiÅŸmez
```sql
-- HiÃ§ deÄŸiÅŸmez (dogum_tarihi gibi)
```

#### Type 1: Ãœzerine Yaz
```sql
-- Eski deÄŸer kaybolur
UPDATE dim_customer
SET email = 'new@example.com'
WHERE customer_key = 123;
```

#### Type 2: TarihÃ§e Tut (En YaygÄ±n)
```sql
-- Eski kayÄ±t kapat, yeni kayÄ±t ekle
UPDATE dim_customer
SET valid_to = CURRENT_DATE, is_current = FALSE
WHERE customer_key = 123 AND is_current = TRUE;

INSERT INTO dim_customer (
    customer_id, email, valid_from, valid_to, is_current
) VALUES (
    12345, 'new@example.com', CURRENT_DATE, '9999-12-31', TRUE
);
```

#### Type 3: Ã–nceki DeÄŸeri Sakla
```sql
ALTER TABLE dim_customer 
ADD COLUMN previous_email VARCHAR(100);

UPDATE dim_customer
SET previous_email = email,
    email = 'new@example.com'
WHERE customer_key = 123;
```

---

## 4. ETL vs ELT

### 4.1 ETL (Extract, Transform, Load)

**Geleneksel YaklaÅŸÄ±m:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source    â”‚
â”‚   (OLTP)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Extract
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Staging Area â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Transform
       â”‚ (ETL Tool)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Warehouseâ”‚
â”‚    (OLAP)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Extract (Ã‡Ä±karma)
```python
import psycopg2
import pandas as pd

def extract_from_source():
    # OLTP veritabanÄ±ndan veri Ã§ek
    conn = psycopg2.connect("dbname=oltp_db user=user password=pass")
    
    query = """
        SELECT order_id, customer_id, order_date, total_amount
        FROM orders
        WHERE order_date >= CURRENT_DATE - INTERVAL '1 day'
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    return df
```

#### Transform (DÃ¶nÃ¼ÅŸtÃ¼rme)
```python
def transform_data(df):
    # Veri temizleme
    df = df.dropna()
    
    # Veri tipi dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    df['order_date'] = pd.to_datetime(df['order_date'])
    
    # Ä°ÅŸ kurallarÄ± uygulama
    df['discount_amount'] = df['total_amount'] * 0.1
    df['final_amount'] = df['total_amount'] - df['discount_amount']
    
    # Aggregation
    df_agg = df.groupby(['customer_id', 'order_date']).agg({
        'order_id': 'count',
        'final_amount': 'sum'
    }).reset_index()
    
    return df_agg
```

#### Load (YÃ¼kleme)
```python
def load_to_warehouse(df):
    # Veri ambarÄ±na yÃ¼kle
    conn = psycopg2.connect("dbname=dwh user=user password=pass")
    cursor = conn.cursor()
    
    for _, row in df.iterrows():
        cursor.execute("""
            INSERT INTO fact_daily_sales 
            (customer_key, date_key, order_count, total_revenue)
            VALUES (%s, %s, %s, %s)
        """, (row['customer_id'], row['order_date'], 
              row['order_id'], row['final_amount']))
    
    conn.commit()
    conn.close()
```

#### ETL AvantajlarÄ±
```
âœ… Veri warehouse'a temiz veri gelir
âœ… Network trafiÄŸi az
âœ… Transformation mantÄ±ÄŸÄ± merkezi
âŒ ETL server yÃ¼kÃ¼
âŒ Esneklik dÃ¼ÅŸÃ¼k
```

### 4.2 ELT (Extract, Load, Transform)

**Modern YaklaÅŸÄ±m:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source    â”‚
â”‚   (OLTP)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Extract
       â†“ Load (Raw)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Lake/   â”‚
â”‚  Warehouse   â”‚
â”‚  Transform   â”‚ â† SQL/dbt
â”‚  (Inside)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Modern ELT ile dbt Ã–rneÄŸi

```sql
-- models/staging/stg_orders.sql
WITH source AS (
    SELECT * FROM {{ source('raw', 'orders') }}
),

cleaned AS (
    SELECT
        order_id,
        customer_id,
        CAST(order_date AS DATE) as order_date,
        CAST(total_amount AS DECIMAL(10,2)) as total_amount,
        CURRENT_TIMESTAMP as loaded_at
    FROM source
    WHERE total_amount > 0
)

SELECT * FROM cleaned;

-- models/marts/fct_daily_sales.sql
WITH daily_orders AS (
    SELECT
        customer_id,
        DATE(order_date) as order_date,
        COUNT(*) as order_count,
        SUM(total_amount) as total_revenue
    FROM {{ ref('stg_orders') }}
    GROUP BY 1, 2
)

SELECT * FROM daily_orders;
```

#### ELT AvantajlarÄ±
```
âœ… Data warehouse gÃ¼cÃ¼nÃ¼ kullanÄ±r
âœ… Esnek (SQL ile transform)
âœ… HÄ±zlÄ± data loading
âœ… Raw data saklanÄ±r
âŒ Warehouse compute maliyeti
```

### 4.3 ETL vs ELT KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | ETL | ELT |
|---------|-----|-----|
| **Transform Yeri** | ETL server | Data Warehouse |
| **HÄ±z** | YavaÅŸ | HÄ±zlÄ± |
| **Esneklik** | DÃ¼ÅŸÃ¼k | YÃ¼ksek |
| **Maliyet** | ETL tool lisansÄ± | Warehouse compute |
| **Uygun** | On-premise | Cloud |
| **AraÃ§lar** | Informatica, Talend | dbt, Fivetran |

---

## 5. Veri GÃ¶lÃ¼ (Data Lake)

### 5.1 TanÄ±m

**Data Lake:** Ham verilerin her tÃ¼rlÃ¼ formatÄ±nda saklandÄ±ÄŸÄ± merkezi depo. Schema-on-read yaklaÅŸÄ±mÄ±.

```
Data Warehouse: Schema-on-write
Data Lake: Schema-on-read
```
```
Data Lake Mimarisi
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Data Sources                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  IoT  â”‚  Logs  â”‚  Social  â”‚  Database  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
     â”‚         â”‚        â”‚          â”‚
     â–¼         â–¼        â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Ingestion Layer (Kafka, Nifi)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Raw Zone (Bronze)                â”‚
â”‚    MinIO / S3 - Ham veriler              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Processing Layer (Spark)            â”‚
â”‚    Temizleme, Filtreleme, DÃ¶nÃ¼ÅŸtÃ¼rme    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
         â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Processed    â”‚ â”‚  Curated Zone   â”‚
â”‚ Zone (Silver)â”‚ â”‚  (Gold)         â”‚
â”‚ Temiz verilerâ”‚ â”‚  Analytics-readyâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Data Warehouse vs Data Lake

| Ã–zellik | Data Warehouse | Data Lake |
|---------|---------------|-----------|
| **Veri TÃ¼rÃ¼** | YapÄ±sal | TÃ¼m tÃ¼rler |
| **Åema** | Ã–nceden tanÄ±mlÄ± | Ä°htiyaÃ§ anÄ±nda |
| **Maliyet** | YÃ¼ksek | DÃ¼ÅŸÃ¼k |
| **KullanÄ±cÄ±** | Ä°ÅŸ analistleri | Data scientists |
| **AmaÃ§** | BI, raporlama | ML, keÅŸif |
| **Ä°ÅŸleme** | Batch | Batch + Stream |
| **Storage** | PahalÄ± | Object storage (ucuz) |

### 5.3 Medallion Architecture (Bronze, Silver, Gold)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BRONZE    â”‚  Raw Data (OlduÄŸu gibi)
â”‚  (Raw Zone)  â”‚  - JSON, CSV, Parquet
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Kaynak sistemden aynen
       â”‚
       â†“ Cleaning, Validation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SILVER    â”‚  TemizlenmiÅŸ, Validate
â”‚ (Curated)    â”‚  - Standart format
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Veri kalitesi kontrollÃ¼
       â”‚
       â†“ Business Logic, Aggregation
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     GOLD     â”‚  Ä°ÅŸ deÄŸeri yaratan
â”‚  (Business)  â”‚  - Aggregate tables
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Feature stores
```

#### MinIO ile Data Lake Ã–rneÄŸi

**Docker ile BaÅŸlatma:**
```bash
docker-compose up -d minio minio-client
```

**Python ile Veri YÃ¼kleme:**
```python
from minio import Minio
from datetime import datetime
import json

# MinIO client
client = Minio(
    "localhost:9000",
    access_key="minioadmin",
    secret_key="minioadmin",
    secure=False
)

# BRONZE: Ham veri yÃ¼kle
def upload_to_bronze(data, filename):
    bucket = "bronze"
    
    # JSON olarak kaydet
    json_data = json.dumps(data)
    
    client.put_object(
        bucket,
        f"raw-data/{datetime.now().strftime('%Y/%m/%d')}/{filename}",
        data=json_data.encode('utf-8'),
        length=len(json_data),
        content_type='application/json'
    )

# SILVER: TemizlenmiÅŸ veri
def upload_to_silver(df, filename):
    import io
    
    # Parquet formatÄ±nda kaydet
    buffer = io.BytesIO()
    df.to_parquet(buffer, index=False)
    buffer.seek(0)
    
    client.put_object(
        "silver",
        f"cleaned-data/{filename}.parquet",
        data=buffer,
        length=buffer.getbuffer().nbytes,
        content_type='application/octet-stream'
    )

# GOLD: Aggregate veri
def upload_to_gold(df, table_name):
    import io
    
    buffer = io.BytesIO()
    df.to_parquet(buffer, index=False)
    buffer.seek(0)
    
    client.put_object(
        "gold",
        f"business-tables/{table_name}.parquet",
        data=buffer,
        length=buffer.getbuffer().nbytes,
        content_type='application/octet-stream'
    )
```

### 5.4 Data Lakehouse

**TanÄ±m:** Data Lake + Data Warehouse = Lakehouse

```
Data Lake'in esnekliÄŸi
    +
Data Warehouse'un performansÄ±
    =
Data Lakehouse
```

#### Lakehouse Teknolojileri

**Delta Lake (Databricks):**
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Delta Lake Example") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.0.0") \
    .getOrCreate()

# Delta table oluÅŸtur
df.write.format("delta").mode("overwrite").save("/mnt/delta/events")

# ACID transactions
df.write.format("delta").mode("append").save("/mnt/delta/events")

# Time travel
df_historical = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .load("/mnt/delta/events")

# Schema evolution
df.write.format("delta") \
    .option("mergeSchema", "true") \
    .mode("append") \
    .save("/mnt/delta/events")
```

**Apache Iceberg:**
```sql
-- Iceberg table oluÅŸtur
CREATE TABLE events (
    event_id BIGINT,
    user_id BIGINT,
    event_type STRING,
    timestamp TIMESTAMP
) USING iceberg
PARTITIONED BY (days(timestamp));

-- Time travel
SELECT * FROM events
TIMESTAMP AS OF '2025-01-01 00:00:00';

-- Schema evolution
ALTER TABLE events ADD COLUMN device_type STRING;
```

---

## 6. Modern Veri Mimarileri

### 6.1 Lambda Architecture

**TanÄ±m:** Batch + Speed layer ile hem tarihsel hem gerÃ§ek zamanlÄ± analiz

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sources   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â”‚       â”‚
   â†“       â†“
BATCH    SPEED
Layer    Layer
(Hadoop) (Storm/Flink)
   â”‚       â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Serving    â”‚
â”‚   Layer     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Batch Layer
```python
# Spark batch processing
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("BatchLayer").getOrCreate()

# TÃ¼m tarihsel veriyi iÅŸle
df = spark.read.parquet("s3://data-lake/events/")

# Aggregation
daily_stats = df.groupBy("date", "user_id") \
    .agg({
        "event_count": "sum",
        "revenue": "sum"
    })

# Batch view'a yaz
daily_stats.write.mode("overwrite").parquet("s3://batch-views/daily_stats")
```

#### Speed Layer
```python
# Kafka + Flink for streaming
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

# Kafka'dan oku
t_env.execute_sql("""
    CREATE TABLE events (
        event_id BIGINT,
        user_id BIGINT,
        amount DECIMAL(10,2),
        event_time TIMESTAMP(3)
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'events',
        'properties.bootstrap.servers' = 'localhost:9092'
    )
""")

# GerÃ§ek zamanlÄ± aggregation
t_env.execute_sql("""
    CREATE TABLE realtime_stats AS
    SELECT 
        TUMBLE_START(event_time, INTERVAL '1' MINUTE) as window_start,
        user_id,
        COUNT(*) as event_count,
        SUM(amount) as total_amount
    FROM events
    GROUP BY TUMBLE(event_time, INTERVAL '1' MINUTE), user_id
""")
```

#### Serving Layer
```python
# Batch ve Speed sonuÃ§larÄ±nÄ± birleÅŸtir
def get_user_stats(user_id, date):
    # Batch view'dan oku (tarihsel)
    batch_stats = read_from_batch_view(user_id, date)
    
    # Speed layer'dan oku (son dakika)
    realtime_stats = read_from_speed_layer(user_id)
    
    # BirleÅŸtir
    total_events = batch_stats['event_count'] + realtime_stats['event_count']
    total_revenue = batch_stats['revenue'] + realtime_stats['revenue']
    
    return {
        'event_count': total_events,
        'revenue': total_revenue
    }
```

### 6.2 Kappa Architecture

**TanÄ±m:** Sadece streaming (Lambda'nÄ±n basitleÅŸtirilmiÅŸ versiyonu)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sources   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streaming  â”‚
â”‚   Layer     â”‚
â”‚ (Kafka/Flink)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Serving    â”‚
â”‚   Layer     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantajlar:**
- âœ… Tek kod tabanÄ±
- âœ… Basit mimari
- âœ… GerÃ§ek zamanlÄ±

**Dezavantajlar:**
- âŒ Tarihsel veri yeniden iÅŸleme zor
- âŒ Debugging karmaÅŸÄ±k

### 6.3 Data Mesh

**TanÄ±m:** Domain-oriented, decentralized data ownership

#### Temel Prensipler

**1. Domain Ownership**
```
âŒ Merkezi veri ekibi tÃ¼m verileri yÃ¶netir
âœ… Her domain kendi verisini yÃ¶netir

Sales Domain â†’ Sales Data Product
Marketing Domain â†’ Marketing Data Product
Product Domain â†’ Product Data Product
```

**2. Data as a Product**
```
Veri bir Ã¼rÃ¼n gibi ele alÄ±nÄ±r:
- Kalite garantisi
- DokÃ¼mantasyon
- SLA
- API/Interface
```

**3. Self-serve Platform**
```
Her domain kendi veri altyapÄ±sÄ±nÄ± yÃ¶netir:
- Kendi database'i
- Kendi pipeline'Ä±
- Kendi quality checks
```

**4. Federated Governance**
```
Merkezi standartlar + Domain Ã¶zgÃ¼rlÃ¼ÄŸÃ¼:
- Veri formatlarÄ± (JSON, Parquet)
- GÃ¼venlik standartlarÄ±
- Privacy regulations
- Metadata standards
```

### 6.4 Modern Data Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Data Sources                 â”‚
â”‚  (Databases, APIs, SaaS, Files)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Ingestion (EL)                  â”‚
â”‚  Fivetran, Airbyte, Stitch           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Storage & Compute                 â”‚
â”‚  Snowflake, BigQuery, Databricks     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Transformation (T)                â”‚
â”‚  dbt (data build tool)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BI & Analytics                    â”‚
â”‚  Looker, Tableau, Power BI           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Veri Kalitesi ve YÃ¶netiÅŸimi

### 7.1 Veri Kalitesi BoyutlarÄ±

#### 1. Accuracy (DoÄŸruluk)
```python
# Email format kontrolÃ¼
def check_email_accuracy(df):
    import re
    email_pattern = r'^[\w\.-]+@[\w\.-]+\.\w+
    
    df['email_valid'] = df['email'].apply(
        lambda x: bool(re.match(email_pattern, str(x)))
    )
    
    accuracy_rate = df['email_valid'].mean() * 100
    return accuracy_rate
```

#### 2. Completeness (TamlÄ±k)
```python
# Eksik veri kontrolÃ¼
def check_completeness(df):
    completeness = {}
    
    for col in df.columns:
        null_count = df[col].isnull().sum()
        total_count = len(df)
        completeness[col] = ((total_count - null_count) / total_count) * 100
    
    return completeness
```

#### 3. Consistency (TutarlÄ±lÄ±k)
```python
# TutarlÄ±lÄ±k kontrolÃ¼
def check_consistency(df):
    # YaÅŸ ve doÄŸum tarihi tutarlÄ± mÄ±?
    from datetime import datetime
    
    current_year = datetime.now().year
    df['calculated_age'] = current_year - df['birth_year']
    df['age_consistent'] = abs(df['age'] - df['calculated_age']) <= 1
    
    consistency_rate = df['age_consistent'].mean() * 100
    return consistency_rate
```

#### 4. Timeliness (ZamanlÄ±lÄ±k)
```python
# Veri gÃ¼ncel mi?
def check_timeliness(df):
    from datetime import datetime, timedelta
    
    current_time = datetime.now()
    df['is_fresh'] = (current_time - df['updated_at']) < timedelta(days=1)
    
    timeliness_rate = df['is_fresh'].mean() * 100
    return timeliness_rate
```

#### 5. Uniqueness (Benzersizlik)
```python
# Tekrar eden kayÄ±tlar
def check_uniqueness(df, key_columns):
    total_records = len(df)
    unique_records = df[key_columns].drop_duplicates().shape[0]
    
    uniqueness_rate = (unique_records / total_records) * 100
    
    duplicates = df[df.duplicated(subset=key_columns, keep=False)]
    
    return uniqueness_rate, duplicates
```

### 7.2 Data Quality Framework

```python
class DataQualityChecker:
    def __init__(self, df):
        self.df = df
        self.results = {}
    
    def check_all(self):
        """TÃ¼m kalite kontrollerini Ã§alÄ±ÅŸtÄ±r"""
        self.results['completeness'] = self.check_completeness()
        self.results['accuracy'] = self.check_accuracy()
        self.results['consistency'] = self.check_consistency()
        self.results['uniqueness'] = self.check_uniqueness()
        
        return self.results
    
    def check_completeness(self):
        """Eksik veri oranÄ±"""
        missing_pct = (self.df.isnull().sum() / len(self.df)) * 100
        return missing_pct.to_dict()
    
    def check_accuracy(self):
        """Veri formatÄ± doÄŸruluÄŸu"""
        accuracy = {}
        
        # Email kontrolÃ¼
        if 'email' in self.df.columns:
            import re
            pattern = r'^[\w\.-]+@[\w\.-]+\.\w+
            accuracy['email'] = self.df['email'].apply(
                lambda x: bool(re.match(pattern, str(x))) if pd.notna(x) else False
            ).mean() * 100
        
        return accuracy
    
    def check_consistency(self):
        """MantÄ±ksal tutarlÄ±lÄ±k"""
        issues = []
        
        # Negatif fiyat kontrolÃ¼
        if 'price' in self.df.columns:
            negative_prices = (self.df['price'] < 0).sum()
            if negative_prices > 0:
                issues.append(f"Negatif fiyat: {negative_prices} kayÄ±t")
        
        # Gelecek tarih kontrolÃ¼
        date_columns = self.df.select_dtypes(include=['datetime64']).columns
        for col in date_columns:
            future_dates = (self.df[col] > pd.Timestamp.now()).sum()
            if future_dates > 0:
                issues.append(f"{col}: {future_dates} gelecek tarih")
        
        return issues
    
    def check_uniqueness(self):
        """Tekrar eden kayÄ±tlar"""
        duplicates = self.df.duplicated().sum()
        return {
            'duplicate_count': duplicates,
            'duplicate_pct': (duplicates / len(self.df)) * 100
        }
    
    def generate_report(self):
        """DetaylÄ± rapor oluÅŸtur"""
        self.check_all()
        
        print("=" * 50)
        print("VERÄ° KALÄ°TESÄ° RAPORU")
        print("=" * 50)
        
        print("\n1. COMPLETENESS (TamlÄ±k)")
        for col, pct in self.results['completeness'].items():
            status = "âœ…" if pct < 5 else "âš ï¸" if pct < 20 else "âŒ"
            print(f"{status} {col}: %{100-pct:.2f} dolu")
        
        print("\n2. ACCURACY (DoÄŸruluk)")
        for field, pct in self.results['accuracy'].items():
            status = "âœ…" if pct > 95 else "âš ï¸" if pct > 80 else "âŒ"
            print(f"{status} {field}: %{pct:.2f} doÄŸru")
        
        print("\n3. CONSISTENCY (TutarlÄ±lÄ±k)")
        if self.results['consistency']:
            for issue in self.results['consistency']:
                print(f"âŒ {issue}")
        else:
            print("âœ… TutarlÄ±lÄ±k sorunu yok")
        
        print("\n4. UNIQUENESS (Benzersizlik)")
        dup = self.results['uniqueness']
        if dup['duplicate_count'] == 0:
            print("âœ… Tekrar eden kayÄ±t yok")
        else:
            print(f"âŒ {dup['duplicate_count']} tekrar eden kayÄ±t "
                  f"(%{dup['duplicate_pct']:.2f})")

# KullanÄ±m
df = pd.read_csv('data.csv')
checker = DataQualityChecker(df)
checker.generate_report()
```

### 7.3 Data Governance

#### Temel BileÅŸenler

**1. Data Catalog**
```yaml
# Metadata yÃ¶netimi
dataset:
  name: customer_transactions
  description: Daily customer transaction data
  owner: sales-team@company.com
  tags: [pii, financial, daily]
  
  schema:
    - name: customer_id
      type: bigint
      description: Unique customer identifier
      pii: false
    
    - name: email
      type: string
      description: Customer email address
      pii: true
      sensitivity: high
    
    - name: transaction_amount
      type: decimal(10,2)
      description: Transaction amount in TRY
      pii: false
  
  quality_rules:
    - field: email
      rule: email_format
      threshold: 95
    
    - field: transaction_amount
      rule: positive_value
      threshold: 100
  
  access_control:
    - role: analyst
      permissions: [read]
    - role: data_engineer
      permissions: [read, write]
```

**2. Data Lineage**
```python
# Veri akÄ±ÅŸÄ±nÄ± izleme
lineage = {
    "dataset": "gold.customer_metrics",
    "upstream": [
        {
            "dataset": "silver.customers",
            "transformations": ["deduplication", "validation"]
        },
        {
            "dataset": "silver.transactions",
            "transformations": ["aggregation", "filtering"]
        }
    ],
    "transformations": [
        "join on customer_id",
        "group by customer_id",
        "calculate lifetime_value"
    ],
    "downstream": [
        "bi_dashboard.customer_360",
        "ml_model.churn_prediction"
    ]
}
```

**3. Data Security**
```sql
-- SatÄ±r seviyesi gÃ¼venlik (Row-Level Security)
CREATE POLICY sales_rep_policy ON orders
FOR SELECT
TO sales_role
USING (sales_rep_id = current_user_id());

-- SÃ¼tun seviyesi ÅŸifreleme
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    name VARCHAR(100),
    ssn VARCHAR(11) ENCRYPTED,  -- Åifreli
    email VARCHAR(100)
);

-- Maskeleme (Data Masking)
CREATE VIEW customers_masked AS
SELECT 
    customer_id,
    name,
    CONCAT('***-**-', RIGHT(ssn, 4)) as ssn_masked,
    CONCAT(LEFT(email, 3), '***@', SPLIT_PART(email, '@', 2)) as email_masked
FROM customers;
```

---

## 8. Veri GÃ¶rselleÅŸtirme

### 8.1 GÃ¶rselleÅŸtirme Prensipleri

#### 1. DoÄŸru Grafik SeÃ§imi

**KarÅŸÄ±laÅŸtÄ±rma:** Bar Chart
**Trend:** Line Chart
**DaÄŸÄ±lÄ±m:** Scatter Plot
**Oran:** Pie Chart (dikkatli kullan!)
**Ä°liÅŸki:** Scatter Plot, Heatmap

#### 2. Renk KullanÄ±mÄ±
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Colorblind-friendly palette
sns.set_palette("colorblind")

# Diverging colormap (pozitif-negatif iÃ§in)
colors = sns.diverging_palette(10, 133, as_cmap=True)
```

#### 3. Eksen ve Label'lar
```python
fig, ax = plt.subplots(figsize=(12, 6))

ax.plot(dates, values)

# Clear labels
ax.set_xlabel('Tarih', fontsize=12)
ax.set_ylabel('SatÄ±ÅŸ (TRY)', fontsize=12)
ax.set_title('AylÄ±k SatÄ±ÅŸ Trendi - 2024', fontsize=14, fontweight='bold')

# Format axis
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'â‚º{x:,.0f}'))

# Grid
ax.grid(True, alpha=0.3)

plt.tight_layout()
```

### 8.2 Python ile Dashboard

```python
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd

# Veri hazÄ±rlama
df = pd.read_csv('sales_data.csv')

# Dashboard layout
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('AylÄ±k SatÄ±ÅŸ Trendi', 'Kategori DaÄŸÄ±lÄ±mÄ±',
                    'BÃ¶lgesel Performans', 'Top 10 ÃœrÃ¼n'),
    specs=[[{"type": "scatter"}, {"type": "pie"}],
           [{"type": "bar"}, {"type": "bar"}]]
)

# 1. Trend chart
monthly_sales = df.groupby('month')['revenue'].sum()
fig.add_trace(
    go.Scatter(x=monthly_sales.index, y=monthly_sales.values,
               mode='lines+markers', name='SatÄ±ÅŸ'),
    row=1, col=1
)

# 2. Pie chart
category_dist = df.groupby('category')['revenue'].sum()
fig.add_trace(
    go.Pie(labels=category_dist.index, values=category_dist.values),
    row=1, col=2
)

# 3. Bar chart - BÃ¶lgesel
regional_sales = df.groupby('region')['revenue'].sum().sort_values(ascending=False)
fig.add_trace(
    go.Bar(x=regional_sales.index, y=regional_sales.values),
    row=2, col=1
)

# 4. Bar chart - Top products
top_products = df.groupby('product')['revenue'].sum().nlargest(10)
fig.add_trace(
    go.Bar(x=top_products.values, y=top_products.index, orientation='h'),
    row=2, col=2
)

# Layout
fig.update_layout(
    height=800,
    title_text="SatÄ±ÅŸ Dashboard - 2024",
    showlegend=False
)

fig.show()
```

---

## 9. Pratik Uygulamalar

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Temel Servisleri BaÅŸlat

```bash
# Dizine git
cd week4-datawarehouse

# Environment deÄŸiÅŸkenlerini kopyala
cp .env.example .env

# OLTP ve OLAP baÅŸlat
docker-compose up -d postgres-oltp postgres-olap pgadmin
```

### 2. OLTP Verisini YÃ¼kle

```bash
# Schema ve Ã¶rnek verileri yÃ¼kle
docker exec -i postgres_oltp psql -U oltp_user -d ecommerce_oltp < oltp/init/01-schema.sql
docker exec -i postgres_oltp psql -U oltp_user -d ecommerce_oltp < oltp/init/02-sample-data.sql
```

### 3. OLAP YapÄ±sÄ±nÄ± OluÅŸtur

```bash
# Dimension ve Fact tablolarÄ±nÄ± oluÅŸtur
docker exec -i postgres_olap psql -U olap_user -d ecommerce_olap < olap/init/01-dimensions.sql
docker exec -i postgres_olap psql -U olap_user -d ecommerce_olap < olap/init/02-facts.sql
docker exec -i postgres_olap psql -U olap_user -d ecommerce_olap < olap/init/03-star-schema.sql
```

### 4. ETL Pipeline Ã‡alÄ±ÅŸtÄ±r

```bash
# ETL servisini baÅŸlat
docker-compose --profile etl up -d etl-service

# ETL'i manuel Ã§alÄ±ÅŸtÄ±r
docker exec etl_service python full_pipeline.py
```

### 5. Data Lake OluÅŸtur

```bash
# MinIO baÅŸlat
docker-compose up -d minio minio-init

# MinIO Console aÃ§
open http://localhost:9001
# GiriÅŸ: minio_admin / minio_password123
```

### 6. Spark ile Veri Ä°ÅŸle

```bash
# Spark baÅŸlat
docker-compose up -d spark-master spark-worker

# Spark UI aÃ§
open http://localhost:8080

# Spark job Ã§alÄ±ÅŸtÄ±r
docker exec spark_master spark-submit /opt/spark-jobs/batch_processing.py
```

### 7. BI Dashboard OluÅŸtur

```bash
# Superset baÅŸlat
docker-compose up -d superset

# Superset aÃ§ (2-3 dakika bekleyin)
open http://localhost:8088
# GiriÅŸ: admin / admin123
```

## ğŸ“Š Servisler ve EriÅŸim

### Web ArayÃ¼zleri

| Servis | URL | KullanÄ±cÄ± | Åifre |
|--------|-----|-----------|-------|
| pgAdmin | http://localhost:5050 | admin@datawarehouse.com | admin123 |
| MinIO Console | http://localhost:9001 | minio_admin | minio_password123 |
| Spark Master UI | http://localhost:8080 | - | - |
| Jupyter Lab | http://localhost:8888 | - | token: datawarehouse123 |
| Superset | http://localhost:8088 | admin | admin123 |
| Airflow | http://localhost:8089 | airflow | airflow123 |
| Metabase | http://localhost:3000 | - | Ä°lk kurulum |

### VeritabanÄ± BaÄŸlantÄ±larÄ±

**OLTP PostgreSQL:**
```bash
Host: localhost
Port: 5432
Database: ecommerce_oltp
User: oltp_user
Password: oltp_pass

# BaÄŸlan:
docker exec -it postgres_oltp psql -U oltp_user -d ecommerce_oltp
```

**OLAP PostgreSQL:**
```bash
Host: localhost
Port: 5433
Database: ecommerce_olap
User: olap_user
Password: olap_pass

# BaÄŸlan:
docker exec -it postgres_olap psql -U olap_user -d ecommerce_olap
```


### Komple ETL Pipeline Ã–rneÄŸi

```python
import pandas as pd
import psycopg2
from datetime import datetime

class ETLPipeline:
    def __init__(self):
        self.oltp_conn = psycopg2.connect(
            "host=localhost port=5432 dbname=oltp_db user=veri_user password=veri_pass"
        )
        self.olap_conn = psycopg2.connect(
            "host=localhost port=5433 dbname=olap_db user=veri_user password=veri_pass"
        )
    
    def extract(self):
        """OLTP'den veri Ã§ek"""
        print(f"[{datetime.now()}] Extracting data from OLTP...")
        
        query = """
            SELECT 
                o.order_id,
                o.customer_id,
                o.order_date,
                oi.product_id,
                oi.quantity,
                oi.unit_price,
                oi.quantity * oi.unit_price as line_total
            FROM orders o
            JOIN order_items oi ON o.order_id = oi.order_id
            WHERE o.order_date >= CURRENT_DATE - INTERVAL '1 day'
        """
        
        df = pd.read_sql(query, self.oltp_conn)
        print(f"Extracted {len(df)} records")
        
        return df
    
    def transform(self, df):
        """Veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼"""
        print(f"[{datetime.now()}] Transforming data...")
        
        # Temizleme
        df = df.dropna()
        
        # Tarih parÃ§alama
        df['order_date'] = pd.to_datetime(df['order_date'])
        df['year'] = df['order_date'].dt.year
        df['month'] = df['order_date'].dt.month
        df['day'] = df['order_date'].dt.day
        
        # Aggregation
        daily_summary = df.groupby([
            'order_date', 'customer_id', 'product_id'
        ]).agg({
            'quantity': 'sum',
            'line_total': 'sum'
        }).reset_index()
        
        print(f"Transformed to {len(daily_summary)} records")
        
        return daily_summary
    
    def load(self, df):
        """OLAP'a yÃ¼kle"""
        print(f"[{datetime.now()}] Loading data to OLAP...")
        
        cursor = self.olap_conn.cursor()
        
        for _, row in df.iterrows():
            cursor.execute("""
                INSERT INTO fact_daily_sales 
                (date_key, customer_key, product_key, quantity, revenue)
                VALUES (%s, %s, %s, %s, %s)
                ON CONFLICT (date_key, customer_key, product_key) 
                DO UPDATE SET
                    quantity = EXCLUDED.quantity,
                    revenue = EXCLUDED.revenue
            """, (
                row['order_date'].strftime('%Y%m%d'),
                row['customer_id'],
                row['product_id'],
                row['quantity'],
                row['line_total']
            ))
        
        self.olap_conn.commit()
        print(f"Loaded {len(df)} records")
    
    def run(self):
        """Pipeline'Ä± Ã§alÄ±ÅŸtÄ±r"""
        try:
            # Extract
            df = self.extract()
            
            # Transform
            df_transformed = self.transform(df)
            
            # Load
            self.load(df_transformed)
            
            print(f"[{datetime.now()}] ETL pipeline completed successfully!")
            
        except Exception as e:
            print(f"[{datetime.now()}] ETL pipeline failed: {str(e)}")
            raise
        
        finally:
            self.oltp_conn.close()
            self.olap_conn.close()

# Ã‡alÄ±ÅŸtÄ±r
pipeline = ETLPipeline()
pipeline.run()
```

---

## 10. AlÄ±ÅŸtÄ±rmalar

### AlÄ±ÅŸtÄ±rma 1: Star Schema TasarÄ±mÄ±
Bir online kitap maÄŸazasÄ± iÃ§in star schema tasarlayÄ±n:
- Fact: SatÄ±ÅŸlar
- Dimensions: MÃ¼ÅŸteri, Kitap, Yazar, YayÄ±nevi, Tarih

**[Ã‡Ã¶zÃ¼m](./week4-datawarehouse/exercises/solutions/star-schema-design.sql)**

### AlÄ±ÅŸtÄ±rma 2: ETL Pipeline
OLTP'den OLAP'a gÃ¼nlÃ¼k ETL pipeline'Ä± yazÄ±n.

**[Ã‡Ã¶zÃ¼m](./week4-datawarehouse/exercises/solutions/etl-pipeline.py)**

### AlÄ±ÅŸtÄ±rma 3: Data Quality
Veri kalitesi kontrol scripti yazÄ±n ve rapor oluÅŸturun.

**[Ã‡Ã¶zÃ¼m](./week4-datawarehouse/exercises/solutions/data-quality.py)**

---

**Ã–zet:** Bu haftada OLTP/OLAP ayrÄ±mÄ±nÄ±, veri ambarÄ± mimarisini, boyutsal modellemeyi, ETL sÃ¼reÃ§lerini, data lake kavramÄ±nÄ± ve modern veri mimarilerini Ã¶ÄŸrendik.

**[â† Hafta 3'e DÃ¶n](../week3-nosql/README.md) | [Ana Sayfaya DÃ¶n](../README.md) | [Hafta 5'e Git â†’](../week5-advanced-sql/README.md)**