# Apache Airflow - Workflow Orchestration

Apache Airflow ile ETL pipeline'larÄ±nÄ± schedule etme ve orkestrasyon.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Airflow'u BaÅŸlat

```bash
# Airflow klasÃ¶rÃ¼ne git
cd airflow

# Airflow cluster'Ä± baÅŸlat
docker-compose up -d

# LoglarÄ± izle
docker-compose logs -f

# Durumu kontrol et
docker-compose ps
```

### 2. Web UI'ya EriÅŸ

```
URL: http://localhost:8080
KullanÄ±cÄ±: airflow
Åifre: airflow123
```

### 3. Ä°lk DAG'Ä± Ã‡alÄ±ÅŸtÄ±r

1. Web UI'da **DAGs** sayfasÄ±na git
2. `example_etl_dag` DAG'Ä±nÄ± bul
3. Toggle switch ile aktif et
4. â–¶ï¸ **Trigger DAG** butonuna tÄ±kla
5. **Graph** view'da Ã§alÄ±ÅŸmayÄ± izle

## ğŸ“… DAG'lar (Data Pipelines)

### 1. example_etl_dag.py
Basit ETL Ã¶rneÄŸi - Extract, Transform, Load adÄ±mlarÄ±.

**Schedule:** Her gÃ¼n 02:00  
**Tasks:**
- `extract`: Veri Ã§ekme
- `transform`: Veri dÃ¶nÃ¼ÅŸtÃ¼rme
- `load`: Veri yÃ¼kleme

```bash
# Manuel tetikleme
docker exec airflow-webserver airflow dags trigger example_etl_dag

# Backfill (geÃ§miÅŸ tarih iÃ§in Ã§alÄ±ÅŸtÄ±rma)
docker exec airflow-webserver airflow dags backfill example_etl_dag \
    --start-date 2024-01-01 --end-date 2024-01-07
```

### 2. postgres_to_minio_dag.py
PostgreSQL'den MinIO'ya veri transfer.

**Schedule:** Her gÃ¼n 03:00  
**Tasks:**
- `extract_customers`: MÃ¼ÅŸteri verisi
- `extract_products`: ÃœrÃ¼n verisi
- `extract_orders`: SipariÅŸ verisi
- `upload_to_minio`: MinIO'ya yÃ¼kleme

### 3. daily_sales_dag.py
GÃ¼nlÃ¼k satÄ±ÅŸ raporu oluÅŸturma.

**Schedule:** Her gÃ¼n 04:00  
**Tasks:**
- `calculate_daily_sales`: SatÄ±ÅŸ hesaplama
- `aggregate_by_product`: ÃœrÃ¼n bazÄ±nda toplama
- `save_report`: Rapor kaydetme

## ğŸ’» DAG Yazma

### Basit DAG YapÄ±sÄ±

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Default arguments
default_args = {
    'owner': 'data-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# DAG tanÄ±mla
with DAG(
    dag_id='my_simple_dag',
    default_args=default_args,
    description='Basit Ã¶rnek DAG',
    schedule='0 2 * * *',  # Her gÃ¼n 02:00
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['example'],
) as dag:
    
    def extract_data():
        print("Veri Ã§ekiliyor...")
        return "data"
    
    def transform_data(ti):
        data = ti.xcom_pull(task_ids='extract')
        print(f"Veri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor: {data}")
        return f"transformed_{data}"
    
    def load_data(ti):
        data = ti.xcom_pull(task_ids='transform')
        print(f"Veri yÃ¼kleniyor: {data}")
    
    # Task'larÄ± tanÄ±mla
    extract = PythonOperator(
        task_id='extract',
        python_callable=extract_data,
    )
    
    transform = PythonOperator(
        task_id='transform',
        python_callable=transform_data,
    )
    
    load = PythonOperator(
        task_id='load',
        python_callable=load_data,
    )
    
    # Task sÄ±rasÄ±
    extract >> transform >> load
```

## ğŸ”§ Task Dependencies (BaÄŸÄ±mlÄ±lÄ±klar)

### Linear (DoÄŸrusal)
```python
task1 >> task2 >> task3 >> task4
```

### Parallel (Paralel)
```python
start >> [task1, task2, task3] >> end
```

### Conditional (KoÅŸullu)
```python
from airflow.operators.python import BranchPythonOperator

def choose_branch():
    if condition:
        return 'task_a'
    else:
        return 'task_b'

branch = BranchPythonOperator(
    task_id='branch',
    python_callable=choose_branch,
)

branch >> [task_a, task_b]
```

## ğŸ—„ï¸ Connections (BaÄŸlantÄ±lar)

### PostgreSQL Connection Ekleme

**Web UI'dan:**
1. Admin > Connections > + (Add)
2. Connection Type: `Postgres`
3. Connection Id: `postgres_oltp`
4. Host: `postgres-oltp`
5. Schema: `ecommerce_oltp`
6. Login: `oltp_user`
7. Password: `oltp_pass123`
8. Port: `5432`

**CLI'dan:**
```bash
docker exec airflow-webserver airflow connections add \
    'postgres_oltp' \
    --conn-type 'postgres' \
    --conn-host 'postgres-oltp' \
    --conn-schema 'ecommerce_oltp' \
    --conn-login 'oltp_user' \
    --conn-password 'oltp_pass123' \
    --conn-port 5432
```

### MinIO/S3 Connection
```bash
docker exec airflow-webserver airflow connections add \
    'minio_conn' \
    --conn-type 's3' \
    --conn-host 'http://minio:9000' \
    --conn-login 'minio_admin' \
    --conn-password 'minio_password123'
```

## ğŸ“Š Operators (Task Tipleri)

### PythonOperator
```python
from airflow.operators.python import PythonOperator

task = PythonOperator(
    task_id='my_task',
    python_callable=my_function,
)
```

### PostgresOperator
```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

task = PostgresOperator(
    task_id='query_db',
    postgres_conn_id='postgres_oltp',
    sql='SELECT * FROM customers LIMIT 10;',
)
```

### BashOperator
```python
from airflow.operators.bash import BashOperator

task = BashOperator(
    task_id='run_script',
    bash_command='echo "Hello from Airflow"',
)
```

### EmailOperator
```python
from airflow.operators.email import EmailOperator

task = EmailOperator(
    task_id='send_email',
    to='team@example.com',
    subject='ETL Completed',
    html_content='<p>ETL pipeline baÅŸarÄ±lÄ±!</p>',
)
```

## ğŸ”„ XCom (Task ArasÄ± Veri PaylaÅŸÄ±mÄ±)

### Veri GÃ¶nderme (Push)
```python
def extract(**context):
    data = {"sales": 1000, "customers": 50}
    # XCom'a yaz
    context['ti'].xcom_push(key='sales_data', value=data)
    # Ya da return (otomatik push)
    return data
```

### Veri Alma (Pull)
```python
def transform(**context):
    # XCom'dan oku
    data = context['ti'].xcom_pull(
        task_ids='extract',
        key='sales_data'
    )
    print(f"AlÄ±nan veri: {data}")
```

## â° Schedule Formats

### Cron Expression
```python
schedule='0 2 * * *'     # Her gÃ¼n 02:00
schedule='0 */4 * * *'   # Her 4 saatte bir
schedule='0 0 * * 1'     # Her Pazartesi 00:00
schedule='0 0 1 * *'     # Her ayÄ±n 1'i 00:00
```

### Preset Schedules
```python
from airflow.timetables.trigger import CronTriggerTimetable

schedule='@daily'        # Her gÃ¼n 00:00
schedule='@hourly'       # Her saat baÅŸÄ±
schedule='@weekly'       # Her Pazar 00:00
schedule='@monthly'      # Her ayÄ±n 1'i 00:00
schedule=None            # Manuel tetikleme
```

## ğŸ³ Docker Compose DetaylarÄ±

### Servisler
- **postgres-airflow**: Metadata database
- **airflow-init**: Ä°lk kurulum (one-time)
- **airflow-webserver**: Web UI (port 8080)
- **airflow-scheduler**: DAG scheduler
- **airflow-worker**: Task executor (opsiyonel)

### Executor Tipleri
Bu setup **LocalExecutor** kullanÄ±r (tek worker, basit ve hÄ±zlÄ±).

Production iÃ§in:
- **CeleryExecutor**: DaÄŸÄ±tÄ±k task execution
- **KubernetesExecutor**: Kubernetes Ã¼zerinde

### Volumes
- `./dags`: DAG dosyalarÄ±
- `./logs`: Task loglarÄ±
- `./plugins`: Custom plugins
- `postgres_airflow_data`: PostgreSQL data

## ğŸ” Monitoring ve Debug

### Web UI Features
- **DAGs**: TÃ¼m pipeline'larÄ± listele
- **Graph View**: Task baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± gÃ¶rselleÅŸtir
- **Tree View**: Tarihsel Ã§alÄ±ÅŸmalarÄ± gÃ¶rÃ¼ntÃ¼le
- **Gantt Chart**: Task sÃ¼relerini analiz et
- **Task Logs**: Her task'Ä±n loglarÄ±nÄ± oku

### CLI Commands
```bash
# DAG listesi
docker exec airflow-webserver airflow dags list

# DAG detayÄ±
docker exec airflow-webserver airflow dags show example_etl_dag

# Task listesi
docker exec airflow-webserver airflow tasks list example_etl_dag

# Task test (gerÃ§ek Ã§alÄ±ÅŸtÄ±rma deÄŸil)
docker exec airflow-webserver airflow tasks test example_etl_dag extract 2024-01-01

# DAG test
docker exec airflow-webserver airflow dags test example_etl_dag 2024-01-01
```

### Log Kontrol
```bash
# Webserver loglarÄ±
docker-compose logs -f airflow-webserver

# Scheduler loglarÄ±
docker-compose logs -f airflow-scheduler

# TÃ¼m loglar
docker-compose logs -f
```

## ğŸ¯ Best Practices

### 1. Ä°dempotency (Tekrar Ã‡alÄ±ÅŸtÄ±rÄ±labilirlik)
DAG'lar aynÄ± tarihte tekrar Ã§alÄ±ÅŸtÄ±rÄ±labilmeli:
```python
# âœ… Ä°yi
df.write.mode("overwrite").parquet(path)

# âŒ KÃ¶tÃ¼
df.write.mode("append").parquet(path)
```

### 2. Task Boyutu
- Her task kÃ¼Ã§Ã¼k ve tek bir iÅŸ yapmalÄ±
- 5-15 dakika arasÄ± ideal
- Ã‡ok uzun task'larÄ± bÃ¶l

### 3. Error Handling
```python
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
}
```

### 4. Documentation
```python
dag = DAG(
    dag_id='my_dag',
    description='Bu DAG ÅŸunu yapar...',
    doc_md="""
    ## DetaylÄ± AÃ§Ä±klama
    
    Bu DAG:
    1. PostgreSQL'den veri Ã§eker
    2. Transform eder
    3. MinIO'ya yÃ¼kler
    """,
)
```

### 5. Tags
```python
dag = DAG(
    dag_id='sales_pipeline',
    tags=['etl', 'sales', 'daily', 'production'],
)
```

## ğŸ›‘ Durdurma ve Temizlik

```bash
# Durdur
docker-compose down

# Volume'lerle birlikte sil
docker-compose down -v

# Yeniden baÅŸlat
docker-compose restart

# Sadece scheduler'Ä± yeniden baÅŸlat
docker-compose restart airflow-scheduler
```

## ğŸ”— BaÄŸÄ±mlÄ±lÄ±klar

Week 4 eÄŸitiminin parÃ§asÄ±dÄ±r:
- âœ… PostgreSQL (OLTP/OLAP)
- âœ… MinIO (Data Lake)
- âœ… Spark (opsiyonel)
- âœ… `datawarehouse_network` network

## ğŸ“š Ã–ÄŸrenme KaynaklarÄ±

- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Airflow Concepts](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Airflow Providers](https://airflow.apache.org/docs/apache-airflow-providers/)

## ğŸ’¡ Ä°puÃ§larÄ±

1. **GeliÅŸtirme**: Task'Ä± Ã¶nce `airflow tasks test` ile test et
2. **Debug**: Task loglarÄ±nÄ± Web UI'dan oku
3. **Schedule**: Cron expression'larÄ± test et (crontab.guru)
4. **XCom**: KÃ¼Ã§Ã¼k veriler iÃ§in kullan (<1MB)
5. **Backfill**: Dikkatli kullan, tÃ¼m tarihleri Ã§alÄ±ÅŸtÄ±rÄ±r