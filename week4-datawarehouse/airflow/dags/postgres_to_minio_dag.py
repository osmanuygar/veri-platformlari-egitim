"""
PostgreSQL â†’ MinIO ETL DAG
OLTP database'den veri Ã§ekip MinIO'ya Parquet olarak yÃ¼kler
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime, timedelta
import pandas as pd
from minio import Minio
import logging
import io

logger = logging.getLogger(__name__)

# Default arguments
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# MinIO Configuration
MINIO_ENDPOINT = "minio:9000"
MINIO_ACCESS_KEY = "minio_admin"
MINIO_SECRET_KEY = "minio_password123"
MINIO_BUCKET = "raw-data"


def get_minio_client():
    """MinIO client oluÅŸtur"""
    return Minio(
        MINIO_ENDPOINT,
        access_key=MINIO_ACCESS_KEY,
        secret_key=MINIO_SECRET_KEY,
        secure=False
    )


def extract_from_postgres(table_name, **context):
    """PostgreSQL'den veri Ã§ek"""
    logger.info(f"ğŸ“¥ {table_name} tablosu Ã§ekiliyor...")

    # PostgresHook ile baÄŸlan
    # Not: Connection 'postgres_oltp' Web UI'dan tanÄ±mlanmalÄ±
    hook = PostgresHook(postgres_conn_id='postgres_oltp')

    # SQL query
    sql = f"SELECT * FROM {table_name}"

    # DataFrame'e Ã§evir
    df = hook.get_pandas_df(sql)

    logger.info(f"âœ“ {len(df)} satÄ±r Ã§ekildi")

    # XCom'a kaydet (kÃ¼Ã§Ã¼k veriler iÃ§in)
    context['ti'].xcom_push(key=f'{table_name}_count', value=len(df))

    return df


def upload_to_minio(table_name, **context):
    """MinIO'ya Parquet olarak yÃ¼kle"""
    logger.info(f"ğŸ“¤ {table_name} MinIO'ya yÃ¼kleniyor...")

    # Ã–nceki task'tan veri al
    df = context['ti'].xcom_pull(task_ids=f'extract_{table_name}')

    if df is None or len(df) == 0:
        logger.warning(f"âš  {table_name} iÃ§in veri yok")
        return

    # Parquet buffer'a yaz
    buffer = io.BytesIO()
    df.to_parquet(buffer, engine='pyarrow', compression='snappy', index=False)
    buffer.seek(0)

    # MinIO client
    client = get_minio_client()

    # Bucket kontrolÃ¼
    if not client.bucket_exists(MINIO_BUCKET):
        client.make_bucket(MINIO_BUCKET)
        logger.info(f"âœ“ Bucket oluÅŸturuldu: {MINIO_BUCKET}")

    # Dosya adÄ± (timestamp ile)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    object_name = f"airflow-etl/{table_name}/{timestamp}.parquet"

    # MinIO'ya yÃ¼kle
    client.put_object(
        MINIO_BUCKET,
        object_name,
        buffer,
        length=buffer.getbuffer().nbytes,
        content_type='application/octet-stream'
    )

    logger.info(f"âœ“ YÃ¼klendi: {object_name} ({len(df)} satÄ±r)")

    return object_name


# DAG tanÄ±mÄ±
with DAG(
        dag_id='postgres_to_minio_dag',
        default_args=default_args,
        description='PostgreSQL verilerini MinIO\'ya Parquet olarak yÃ¼kle',
        schedule='0 3 * * *',  # Her gÃ¼n 03:00
        start_date=datetime(2024, 1, 1),
        catchup=False,
        tags=['etl', 'postgres', 'minio', 'parquet'],
        doc_md="""
    ## PostgreSQL â†’ MinIO ETL

    Bu DAG:
    1. PostgreSQL OLTP database'den 3 tablo Ã§eker
    2. Her tabloyu Parquet formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    3. MinIO Data Lake'e yÃ¼kler

    **Tablolar:**
    - customers
    - products
    - orders
    """,
) as dag:
    # Tablolar
    tables = ['customers', 'products', 'orders']

    # Her tablo iÃ§in extract ve upload task'larÄ± oluÅŸtur
    for table in tables:
        extract_task = PythonOperator(
            task_id=f'extract_{table}',
            python_callable=extract_from_postgres,
            op_kwargs={'table_name': table},
        )

        upload_task = PythonOperator(
            task_id=f'upload_{table}',
            python_callable=upload_to_minio,
            op_kwargs={'table_name': table},
        )

        # Task baÄŸÄ±mlÄ±lÄ±ÄŸÄ±
        extract_task >> upload_task